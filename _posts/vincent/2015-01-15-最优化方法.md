---
layout: post
category : 机器学习
tagline: "Supporting tagline"
tags : [最优化]
---
{% include JB/setup %}


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 最优化方法

一般我们接触到的最优化分为两类：

- 无约束最优化
- 有约束最优化

## 无约束最优化

通常对于无约束最优化，首先要判断是否为凸函数。

[无约束最优化](http://www.52nlp.cn/unconstrained-optimization-one)

[机器学习中导数最优化方法](http://www.cnblogs.com/daniel-D/p/3377840.html)

[最大似然、逻辑回归和随机梯度训练](http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf)

### 梯度下降法

- [教程:随机梯度下降(SGD) tricks《Stochastic gradient descent tricks》L Bottou (2012) ](http://t.cn/8kFIzvk)

### 牛顿法

### 拟牛顿法

### 共轭梯度法

## 有约束最优化

一般采用拉格朗日方程，kkt，对偶问题求解。

譬如svm里，最大化几何间隔 max y(wx+b)/\|\|w\|\|

[支持向量机](http://blog.csdn.net/v_july_v/article/details/7624837)

首先写出cost function：min [ 1/2*w^2 + max(0,1-y(wx+b)) ]

可以看出，这是一个有约束的问题，那么就可以用到"拉普拉斯+KKT+对偶"来求解了。

### [关于拉格朗日乘子法与KKT条件](http://www.moozhi.com/topic/show/54a8a261c555c08b3d59d996)

[上确界=最小上界](http://zh.wikipedia.org/wiki/最小上界)

共轭函数描述这个最小上界随着y的变化所变化的情况，使函数值有界的y的取值范围是共轭函数的定义域。不管原函数的凹凸性如何，共轭函数一定是凸函数。

拉格朗日对偶函数g(λ,ν)一定是凹的（不论原始优化问题凹凸性如何）。

原始问题目标函数最优值是p⋆，有如下不等式：

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lagrange_ruo_duiou.png)

拉格朗日对偶问题的最优值记为d⋆，相对于原始问题的最优值p⋆，d⋆≤p⋆，这个关系被称作弱对偶关系。

如果原始问题和对偶问题的最优值相等，则是强对偶关系。

通常情况下强对偶关系并不成立。但是如果原始问题是凸的，并且Slater条件成立的情况下，强对偶条件一定成立。

KKT条件是一组解成为最优解的必要条件，如果原始问题是凸的，则KKT条件也是充分的。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/kkt_condition.png)

上面前两行条件是原始问题的约束，第三行是拉格朗日函数的要求，第四行是互补松弛条件，最后一行是对拉格朗日函数取极值时候带来的一个必要条件。关于这上面的第五个条件，从几何意义上也可以得到比较明确的解释，即目标函数上的梯度下降方向与约束函数的梯度下降方法是平行的。

如果一个凸优化问题有可微的目标函数和约束，并且满足Slater条件，则KKT条件是取得最优的充要条件：Slater条件保证了最优对偶间隙为零并且最优点可以取到；在此基础上x是最优当且仅当(x,λ,ν)满足KKT条件。

KKT条件在优化问题中有重要意义。它可以用于如下方面：

- 有时候可以直接从KKT条件里得到最优的解析解。
- 等式约束的优化问题，可以通过KKT条件转化为无约束方程求零点问题。
- 有不等式约束的优化问题，可以使用KKT条件来简化，帮助求解。

## 最优化算法的并行化

### Logistic Regression

这里主要以Logistic regression为例，讲一讲最优化算法的并行化实现。

先看一下Logistic regression的损失函数：

Logistic函数（或称为Sigmoid函数）为：
$$sigmoid(z)=f(z)=\frac{1}{1+e^{-z}}$$

sigmoid函数的导数为：
$$\nabla f(z)=f(z)(1-f(z))$$

对于线性回归来说，其分类函数为：
$$h(x)=w_0+\sum_{i=1}^p{w_ix_i}=w^Tx$$
其中x是特征向量，w是特征权重向量，p是特征向量维度。

逻辑回归本质上是一个被logistic函数归一化后的线性回归，即在特征到结果的映射中加入了一层sigmoid函数映射。相比于线性回归，模型输出取值范围为[0，1]，

如果y的取值是0或1，定义事件发生的条件概率为：
$$P(y=1|x)=\pi(x)=\frac{1}{1+exp(-h(x))}$$

定义事件不发生的条件概率为：
$$P(y=0|x)=1-P(y=1|x)=\frac{1}{1+exp(h(x))}$$

假设有n个观测样本，分别为：(\\(x_1\\),\\(y_1\\))，(\\(x_2\\),\\(y_2\\)) ... (\\(x_n\\),\\(y_n\\))。

得到一个观测值(\\(x_i\\),\\(y_i\\))的概率为：
$$P(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$$  其中\\(p_i=P(y_i=1|x_i)=\pi(x_i)\\)

由于各项观测独立，所以它们的联合分布可以表示为各边际分布的乘积：
$$l(w)=\prod_{i=1}^n{p_i^{y_i}(1-p_i)^{1-y_i}}$$

对上述函数取对数，根据最大似然估计，得到最优化目标为：
$$\max{L(w)}=\max{log[l(w)]}=\max{\sum_{i=1}^n{y_i*log[\pi(x_i)] + (1-y_i)*log[1-\pi(x_i)]}}$$

对L(w)求导得：
$$\nabla L(w) = \sum_{i=1}^n{(\pi(x_i)-y_i)x_i}$$

而如果y的取值是1或-1，则最优化目标为：

$$\max{L(w)}=\max{\sum_{i=1}^n{-log[1+exp(-y_iw^Tx_i)]}}$$

加上正则项后则是：

$$\min_w \frac{1}{2}||w||^2+C\sum_{i=1}^{n}log[1+exp(-y_iw^Tx_i)]$$

### LR的MapReduce并行

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs.png)

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops.png)

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/lbfgs_two_loops_vf.png)

- [Distributed LIBLINEAR](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/)
- [Large scale learning spotlights](http://nips.cc/Conferences/2014/Program/event.php?ID=4831)

## Loss Function

不同的loss function对应着不同的机器学习算法。

- 0-1 loss: count(y != y')  -> Perceptron
- squared loss: (y-y')^2  -> Linear regression
- exponential loss: exp(-y*w^T*x)  -> Adaboost
- hinge loss: max(0, 1-y*w^T*x)  -> SVM
- cross entropy loss: log2(1 + exp(-y*w^T*x))  -> Logistic regression

更深入一步，hinge loss，exp loss，cross entropy loss都是0-1 loss的上界。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/VC理论/hinge-exp-logistic-bounds.png)

我们可能将关注这样一些问题：为什么线性回归和逻辑回归可以用于做分类？为什么逻辑回归的loss function不取squared loss？SVM的优越之处在哪里？为什么1范数可以得到稀疏解？为什么2范数能得到最大间隔解？正则化与贝叶斯估计的联系？

[loss.pdf](http://web.mit.edu/lrosasco/www/publications/loss.pdf)

[vowpal_wabbit Loss-functions ](https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions)

[Loss function wiki](http://en.wikipedia.org/wiki/Loss_function)

[shark loss function](http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/concepts/library_design/losses.html)

adaboost，svm，lr三个算法的关系：

三种算法的分布对应exponential loss（指数损失函数），hinge loss，log loss（对数损失函数），无本质区别。应用凸上界取代0、1损失，即凸松弛技术。从组合优化到凸集优化问题。凸函数，比较容易计算极值点。

- 关于loss function，记录几点：
	- SVC & SVR：对于SVC，由于hinge loss: max(0, 1-yw'x)，SVC会特别关注那些难分类的点（classify with least certainty; support vectors, i.e., sv）；对于SVR，由于epsilon loss: max(0, \|y-w'x\|-e)，SVR类似的会关注那些residual比较大的点（"outlier"; prediction with least certainty too; sv）
	- SVM和ANN 1) SVM的hinge loss func: max(0, 1 - y'x), 当“激活量” y'x 超过阈值1，那么就不计损失即损失为零 2) Deep Sparse Rectifier的rectifier activation func: max(0, x), 看图它在零点处不可导带来了稀疏性，可大量约简神经网络参数规模 3) loss function + penalty regularization
	- SVM & Boosting：SVM用loss func来剔除了那些容易的点，实际相当于给样本加了权重0，使得模型更加关注难预测的点；Boosting每次迭代后调整权重：降低容易点的权重，增加难预测点的权重，也使得模型更关注难预测的点

- [Who is afraid of non-convex loss functions?](http://www.cs.nyu.edu/~yann/talks/lecun-20071207-nonconvex.pdf)

	Yann LeCun在2007年的讲义。目前大部分loss function都是凸函数，譬如lr，svms，exponential-family graphical models，凸函数一个很优秀的性质，但有些时候却是限制，坚持凸性将增大模型规模或者求解最优化问题方法的复杂程度。

	Deep architectures势必将带来非凸的loss function。但是(1)采用一个合适的模型架构(即便将带来loss function非凸)比坚持凸性更加重要；(2) 即使是对于浅层模型，例如SVM，使用非凸的loss function实际上可以带来准确率和性能的提升。

	对于神经网络，conjugate gradient, BFGS, LM-BFGS没有stochastic gradient好用；对于SVM，“batch” quadratic方法没有SMO好用，而SMO又没有on-line methods好用；对于CRF，Iterative scaling没有stochastic gradient好用；虽说stochastic gradient没有很多的理论保证，但经验证明是不错的选择。

## 最优化算法参考资料

- [几点改进随机梯度下降(SGD)过程的tricks](http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/)
	对梯度本身的探究也是特别有价值的，这里的优化方法，就是让梯度的update更平滑和稳定，记录之前在这个方向上的梯度，做平滑，并防止不同batch带来的突变是有价值的

- [凸优化，convex optimization](http://stanford.edu/~boyd/cvxbook/)

- @王威廉 加州伯克利大学博士 Aria Haghighi 写了一篇超赞的数值优化博文，从牛顿法到拟牛顿法，再到BFGS以及L-BFGS
[Numerical Optimization: Understanding L-BFGS](http://aria42.com/blog/2014/12/understanding-lbfgs/) 图文并茂，还有伪代码，强烈推荐。
@陈如山： 他的这篇文章写的比较精简，我之前也写了几篇关于常用优化算法的文章，从 steepest descent 到 newton 再到 quasi-newton (包括 lbfgs)，包含所有公式推导及部分收敛性分析和 python 代码. [地址1](http://juscodit.github.io)，[地址2](http://blog.csdn.net/itplus/article/details/21896453)

- 比较全面的优化教材 http://t.cn/Rwx3S6S @黄萱菁 发起的讨论 @Copper_PKU: Optimization for Machine Learning（Wright） @夏睿 Numerical Optimization（Nocedal）@wzxhome 推荐Nesterov @程龚_NJU 推荐Dingzhu Du的 @debiandsc 推荐Bertsekas和Ghaoui的

  好像除了这本大家谈的比较多的只有Introductory Lectures on Convex Optimization by Yurii Nesterov http://t.cn/Rwyz6Nb 其他的我也没印象。

  Nocedal 的Numerical Optimization，L-BFGS 算法提出者 http://t.cn/RwUQ40Q

  [Introductory Lectures on Convex Programming](http://enpub.fulton.asu.edu/cseml/Fall2008_ConvOpt/book/Intro-nl.pdf)

  [Numerical_Optimization](http://www.bioinfo.org.cn/~wangchao/maa/Numerical_Optimization.pdf)

  [Optimization for Machine Learning](http://mitpress.mit.edu/books/optimization-machine-learning)

  [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)

  [EFFICIENT METHODS IN CONVEX PROGRAMMING](http://www2.isye.gatech.edu/~nemirovs/Lect_EMCO.pdf)

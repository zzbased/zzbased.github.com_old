---
layout: post
category : 机器学习
tagline: "Supporting tagline"
tags : [deep learning, 神经网络, cnn, rnn, word2vec, image caption]
---
{% include JB/setup %}


## 神经网络基础

- [漫谈ANN(1)：M-P模型](http://hahack.com/reading/ann1/)，[漫谈ANN(2)：BP神经网络](http://hahack.com/reading/ann2/)

	M-P模型:1943年心理学家W.McCulloch和数学家W.Pitts合作提出了这个模型，所以取了他们两个人的名字（McCulloch-Pitts）

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/mp_model.png)

	Perceptron: 在1958年,美国心理学家Frank Rosenblatt,基于M-P模型的结构,提出一种具有单层计算单元的神经网络,称为感知器(Perceptron)。

	单层感知器不能解决非线性问题，譬如"异或"。Kolmogorov理论指出：双隐层感知器就足以解决任何复杂的分类问题。

- [UFLDL-神经网络](http://deeplearning.stanford.edu/wiki/index.php/神经网络), [反向传播算法-详解](http://deeplearning.stanford.edu/wiki/index.php/反向传导算法)

	[为什么多层神经网络是非凸函数](http://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex) 这里有解释，简单点说，如果交换某一层某两个nodes和weights以及相应连接的nodes，将得到a different set of parameters，但是cost function是一样的，所以是非凸的。

	神经网络参数必须随机初始化，而不是全部置为0。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数。随机初始化的目的是使对称失效。

- [神经网络历史](http://blog.sina.com.cn/s/blog_71329a960102v1eo.html)

- [金连文教授-机器学习课程](http://www.hcii-lab.net/lianwen/Course/Machine%20Learning/)，课程中神经网络相关的章节很好。

- [文章]《10 Common Misconceptions about Neural Networks》http://t.cn/RZMtCsP 神经网络十大认识误区，作者是Stuart Gordon Reid，值得一读

- [神经网络训练中的Tricks之高效BP-反向传播算法](http://blog.csdn.net/zouxy09/article/details/45288129) 来自与于Neural Networks: Tricks of the Trade”一书第二版中的第一章Efficient BackProp的部分小节

- [神经网络C++教程](http://t.cn/RwBlP62) 神经网络很好玩。本文介绍了用可调节梯度下降和可调节动量法设计和编码经典BP神经网络，网络经过训练可以做出惊人和美妙的东西出来。要了解更多的知识，可以访问作者的博客：http://t.cn/RwBjPEq

## 深度学习综述

- [Deep Learning Tutorial. LISA lab, University of Montreal](http://deeplearning.net/tutorial/deeplearning.pdf)  [software](http://deeplearning.net/software_links/)
基于Theano的深度学习教程，内容很新，包括了多层感知器，卷积神经网络，auto encoder，RBM，Deep Belief Networks，Monte-carlo sampling，循环神经网络，LSTM，RNN-RBM，Miscellaneous等，值得学习。

- [机器学习(Machine Learning)&深度学习(Deep Learning)资料汇总](http://dataunion.org/8463.html), [第二弹](http://dataunion.org/13920.html)


- [视频]《Neural networks class - Université de Sherbrooke》http://t.cn/8s4WOeb 超棒的神经网络课程，深入浅出介绍深度学习，由Hugo Larochelle（Yoshua Bengio的博士生，Geoffrey Hinton之前的博士后）主讲，强烈推荐！ 云:http://t.cn/RA78nK5 (92节已全部搬至国内，希望对大家有帮助)

- [From feature descriptors to deep learning](http://quantombone.blogspot.com/2015/01/from-feature-descriptors-to-deep.html)

- [What is Deep Learning?](http://simonwinder.com/2015/01/what-is-deep-learning/)

- [Deep Learning 101](http://markus.com/deep-learning-101/)

- [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)

- [deep learning book. by Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/dlbook/)

- [Introduction to Neural Networks and Machine Learning by hinton](http://www.cs.toronto.edu/~tijmen/csc321/) [Lecture notes](http://www.cs.toronto.edu/~tijmen/csc321/lecture_notes.shtml)

- [文章]《Why does Deep Learning work?》http://t.cn/RwYyN5J “Multilayer Neural Networks are just Spin Glasses”

- [谷歌科学家、Hinton亲传弟子Ilya Sutskever的深度学习综述及实际建议](http://weibo.com/p/1001603799166017998138) 比较喜欢其中关于tricks的建议：包括data, preprocessing, minibatches, gradient normalization, learning rate, weight initialization, data augmentation, dropout和ensemble。

- [LeCun：The Unreasonable Effectiveness of Deep Learning](http://www.ee.ucl.ac.uk/sahd2014/resources/LeCun.pdf) LeCun做的300+页的深度学习slides，太棒了！毋需多做介绍，单看标题、作者应该就能作出判断——看看看，必须的！

- [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/) 利用深度学习和维数约减，可以对整个Wikipedia进行可视化，文中结合Wikipedia训练得出的例子，全面介绍了深度学习、词向量、段落向量、翻译模型以及深度学习可视化方面的知识，理论结合实践，实属不可多得的好文。

- [深度学习-笔记整理系列1-8](http://blog.csdn.net/zouxy09/article/details/8775360)，[link2](http://blog.csdn.net/zouxy09/article/details/9993371) 使用自下向上非监督学习（就是从底层开始，一层一层的往顶层训练）；自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）。还介绍了Autoencoder，SparseCoding，Restricted Boltzmann Machine，Deep BeliefNetworks，CNN等模型。

- [博文“A Brief Overview of Deep Learning”](http://t.cn/RZ8Sqe6) 有见解有福利。一些技术总结得不错，例如Practice Advice，有很多干货，谁用谁知道...... 文后还有Bengio的点评及与网友的互动讨论。

- [Bay Area Multimedia Forum](http://www.bammf.org) 邓力，贾扬清，Ronan Collobert, Richard Socher 讲用深度学习处理语音、文本和图像。有slides有视频。

- [Neural network with numpy]() Python下(只)用numpy写神经网络，不错的开始 [github code](https://github.com/FlorianMuellerklein/Machine-Learning/blob/master/BackPropagationNN.py)

- [Google’s Large Scale Deep Neural Networks Project, Greg Corrado](http://techjaw.com/2015/02/21/googles-large-scale-deep-neural-networks-project-greg-corrado/) Google的大规模分布式DNN介绍 [slide](http://pan.baidu.com/s/1kTl76AV) [视频](http://pan.baidu.com/s/1qWmJrSo)

- 《Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning》http://t.cn/Rw981My 做深度学习选择和使用GPU的一些建议

- 对深度学习历史好奇的朋友, 推荐 "The Believers - The hidden story behind the code that runs our lives" http://t.cn/RwCj0wJ "Retrospectively, it was a just a question of the amount of data and the amount of computations," Hinton says. http://t.cn/RwNHl6u

- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)

- Why does Deep Learning work? [part 1](http://t.cn/RAieIie) [part 2](http://t.cn/RAxtm8o)

## Deeplearing in NLP综述

- [深度学习、自然语言处理和表征方法](http://blog.jobbole.com/77709/) [English version](https://github.com/colah/NLP-RNNs-Representations-Post/blob/master/index.md)
	- 单隐层神经网络有一个普适性（universality）：给予足够的隐结点，它可以估算任何函数。普适性的真正意义是：一个网络能适应任何你给它的训练数据。这并不代表插入新的数据点的时候它能表现地很理想。
	- 在深度学习工具箱里，把从任务A中学到的好表征方法用在任务B上是一个很主要的技巧。根据细节不同，这个普遍的技巧的名称也不同，如：预训练（pretraining），迁移学习(transfer learning)，多任务学习(multi-task learning)等。这种方法的好处之一是可以从多种不同数据中学习特征表示。
	- 共享嵌入是一个非常让人兴奋的研究领域，它暗示着为何深度学习中这个注重表征方法的角度是如此的引人入胜。它目前被应用在机器翻译，ImageCaptioning。

## ImageNet classification

- [Googles breakthrough paper shows 10*faster neural nets, and beats a human](http://arxiv.org/pdf/1502.03167v1.pdf) Google的最新论文，用"batch normalization”在ImageNet上得到4.82%( top-5 error)，训练速度也大大加快。

- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/abs/1502.01852) 微软所创建的基于深度卷积神经网络系统首次在ImageNet图像分类上超越人类，实现4.94% top-5 test error。

## Image Caption Generation

- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](http://arxiv.org/abs/1502.03044) 基于视觉焦点用LSTM自动生成图像内容描述。
来自Yoshua Bengio教授团队（22 pages，8页正文+n多附图），文中报道的结果比之前Microsoft、Google的结果更好。

- [Image Captioning](https://pdollar.wordpress.com/2015/01/21/image-captioning/)  来自于微软的一篇博文，介绍了image caption的最近进展。图像标题生成小综述，文中引用了另一篇进展综述性文章《Rapid Progress in Automatic Image Captioning》http://t.cn/Rzzk2H3 ，列举出最有影响和代表性的进展和成果(论文)，并从数据集、验证(评价)和下一步怎么走三方面进行了讨论，观点很有代表性

	[Top Microsoft Machine Learning Posts of 2014](http://t.cn/RZdBZEe) 微软14年最佳博文：图像自动描述生成的飞速进展、机器学习欢乐多、Azule ML为用户带来变革、机器学习与文本分析、微软机器学习20年、Vowpal Wabbit快速学习、什么是机器学习、NIPS14机器学习趋势、机器学习与机器视觉等

- [视频]《Automated Image Captioning with ConvNets and Recurrent Nets》http://t.cn/RwXX7zY Stanford的Andrej Karpathy两周前在SF ML meetup上的报告，基于ConvNets和Recurrent Nets的图像标题自动生成，推荐 云:http://t.cn/RwXXQ07

- [视频]《Deep Learning at Flickr》http://t.cn/RwEqZ5U 介绍深度学习在Flickr的应用，主要是图像标签自动提取 云:http://t.cn/RwEqCi7

- [文章]《One Image Is Worth 1,000 Labels》http://t.cn/RwE0Koi 讨论目前基于深度学习的图片自动标注(类别性描述)在应用方面的局限

- [百度最新力作《基于深度学习的图像识别进展》摘要](http://dataunion.org/14465.html)
	[基于深度学习的图像识别进展](http://www.cvrobot.net/wp-content/uploads/2015/04/都大龙等-基于深度学习的图像识别进展百度的若干实践.pdf)

- 微软研究院大神[Ross Girshick](http://t.cn/RAmNIKx) 发表了题为“Fast R-CNN”的论文。[论文Fast R-CNN](http://arxiv.org/pdf/1504.08083v1.pdf) [项目link](https://github.com/rbgirshick/fast-rcnn)

## RNN& LSTM

- [Gated Feedback Recurrent Neural Networks](http://xxx.tau.ac.il/abs/1502.02367)  又有人设计新的RNN了，这回Cho和Bengio都在．这回介绍的GF-RNN说是能比以前Deep RNN的都好。明摆着说LSTM嘛。

- [论文 Scaling Recurrent Neural Network Language Models》(2015) W Williams, N Prasad, D Mrva](http://t.cn/Rww4fbV) 讨论如何使用GPU训练大型RNN，以及#RNN##语言模型#(RNNLM)在进行扩展时模型大小、训练集规模和运算开销方面的问题；使用维基和新闻语料训练的大规模RNNLM效果明显。

- [Passage：用RNN做文本分析的python库](http://t.cn/RZNfoLt)

- [深入讨论RNN](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)  [译文](http://www.csdn.net/article/2015-01-28/2823747)非常好的讨论递归神经网络的文章，覆盖了RNN的概念、原理、训练及优化等各个方面内容，强烈推荐！本文作者Nikhil Buduma，也是《Deep Learning in a Nutshell》的作者。

- 深度RNN/LSTM用于结构化学习 0)序列标注[Connectionist Temporal Classification ICML06](http://t.cn/RZZs9Io) 1)机器翻译[Sequence to Sequence NIPS14](http://t.cn/RZZs9Jk) 2)成分句法[GRAMMAR AS FOREIGN LANGUAGE](http://t.cn/RZZs9Ia) 再次用到窃取果实distilling

- [视频]《General Sequence Learning using Recurrent Neural Networks》http://t.cn/RwiEI9d Alec Radford讲的用RNN做文本序列分析(学习) 云:http://t.cn/RwinOCb Alec Radford的Passage:http://weibo.com/1402400261/BFVLkxtfw

- [幻灯 Long Short-Term Memory: Tutorial on LSTM Recurrent Networks](http://people.idsia.ch/~juergen/lstm/index.htm)  J. Schmidhuber的LSTM递归网络教程

- [幻灯] 《Theano and LSTM for Sentiment Analysis》http://t.cn/RwofVaF Next.ML 2015上用Theano和LSTM做情感分析的报告幻灯和练习 GitHub:http://t.cn/RwofqDC 云:http://t.cn/Rwofo4i

- 最近很火的两篇文章，Neural Turing Machines和Learning to Execute，都讲到用LSTM RNN做sequence copy，来测试网络保持长时记忆的能力。sequence copy到底怎么定义为一个supervised learning任务呢？和机器翻译一样吗？

- 特别喜欢Long-short-term-memory(LSTM) architecture, it explicitly models 'forgetting' in sequence processing. LSTM是gated RNNs 的典型代表, 它部分解决了 long-term dependency 的问题, Alex Graves 在 LSTM 上较有研究, 读读他的paper: deep RNN for speech recognition http://t.cn/RZt9knn

- Stanford推出利用parsing的树状LSTM, 在Stanford Sentiment Treebank上表现还凑合，句子相似度计算上不错http://t.cn/RwRLwvx

- [深入浅出LSTM](http://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/)《Demystifying LSTM Neural Networks》by Zachary Chase Lipton. pdf:http://t.cn/R22K0KY

## CNN
- [深度卷积网络CNN与图像语义分割](http://blog.csdn.net/xiahouzuoxin/article/details/47789361)

	这个文章写得很好，实践性很强。里面的内容自己之前也都实践过，但是没有这样系统性的记录下来过。caffe卷积层的计算，手绘的网络图，值得学习。

- [深度学习：CNN的反向求导及练习](http://dataunion.org/?p=5395)

- Character-based CNN。[论文 Text Understanding from Scratch 2015 Xiang Zhang, Yann LeCun](http://arxiv.org/abs/1502.01710)  深度学习在NLP领域最新进展！使用temporal ConvNets对大规模文本语料进行学习，在本体分类、情感分析、文本分类任务中取得了“astonishing performance”，不依赖任何语言知识，中英文均适用。必读！

- [论文 A Convolutional Neural Network for Modelling Sentences. Nal Kalchbrenner等](http://arxiv.org/abs/1404.2188) 用动态卷积神经网络(DCNN)对句子进行语义建模，该方法不依赖解析树也不受语种限制，效果也很明显，推荐学习。其项目主页上http://t.cn/RZd9HOE 提供了源码(Matlab) 云:http://t.cn/RZdCx1o

- 用CNN网络实现人脸关键点检测 http://t.cn/RwSwAct | 教你用Theano、Lasagne等工具在python里训练网络检测人脸关键点。跟什么MNIST识别数字相比，这个任务还算是更接近实际问题的复杂程度了。PS：用Lasagne来命名深度网络库还真是吃货本质啊

- [Mnist demos](http://yann.lecun.com/exdb/lenet/index.html)

普通神经网络做图像识别的缺点：
1. 一般要得到较好的训练效果，隐层数目不能太少，当图片大的时候，需要的权值会非常多。
2. 对平移、尺度变化敏感（比如数字偏左上角，右下角时即识别失败）。
3. 图片在相邻区域是相关的，而这种网络只是一股脑把所有像素扔进去，没有考虑图片相关性。

而CNN通过local receptive fields（感受野），shared weights（共享权值），sub-sampling（下采样）概念来解决上述三个问题。[Gradient-Based Learning Applied to Document Recognition ]()

- [卷积神经网络入门](http://blog.csdn.net/stdcoutzyx/article/details/41596663)

	![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts/images/cnn_param_number.png)

	计算卷积神经网络的参数个数，如上图所示，输入是4个通道，输出是2个通道，那么参数个数=2*(4*2*2 + 1)。需要注意的是，输入四个通道上每个通道对应一个卷积核，如上图W参数所示。

- [论文]《Learning a Deep Convolutional Network for Image Super-Resolution》C Dong, CC Loy, K He, X Tang (2014) 用CNN做单副图像升超分辨率(SR)，轻量架构，性能高，可支持在线应用 PDF:http://t.cn/RwrNfBB 源码:http://t.cn/RwrNMNa Kaiming 云:http://t.cn/RwrpUKv

## Word2vec

- [问答]《What are some interesting Word2Vec results?》http://t.cn/RZrOfYp Quora上的主题，讨论Word2Vec的有趣应用，Omer Levy提到了他在CoNLL2014最佳论文里的分析结果和新方法（稍后单独推荐），Daniel Hammack给出了找特异词的小应用并提供了(Python)代码http://t.cn/zQgLQ20

- Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews。[code，include sentence2vec](https://github.com/mesnilgr/iclr15)

- 隐含主题模型LDA的学习过程可为文档每个词分配隐含主题，我组本科生刘扬同学利用LDA为词汇提供的补充信息，提出topical word embeddings，在词汇相似度计算和文本分类上得到一些有趣的结果。[github code](https://github.com/largelymfs/topical_word_embeddings)，[论文Topical Word Embeddings](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_twe.pdf)

- [Wiki2Vec: Generating Vectors for DBpedia Entities via Word2Vec and Wikipedia Dumps](https://github.com/idio/wiki2vec) 从维基百科Dumps生成Word2Vec向量的工具，包括词向量和主题向量

- [Why word2vec works](http://andyljones.tumblr.com/post/111299309808/why-word2vec-works) word2vec的工作原理

- [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf) word2vec梯度推导详解

- [文章]《Movie Review Sentiment Analysis With Word2Vec, DBNs and RNTN》http://t.cn/RwNuyKP Java里用word2vec做电影评论情感分析的例子

- 以word2vec为人所知的前谷歌脑计划研究科学家现在Facebook人工智能实验室的Tomas Mikolov在COLING 2014给的Tutorial 【Using Neural Networks for Modelling and Representing Natural Languages】CBOW，Skip-gram，Negative sampling http://t.cn/RZTjakc Mikolov博士毕业于捷克的布尔诺科技大学

- 我发现word2vec算出来的相似度并没有一个统一的衡量方式，找某个词topN similar是没问题的，但假设有两个pair，并不能因为pair1的word2vec similarity高就说pair1比pair2更相似，因此也没法设一个threshold去取舍这些pairs。
phunter_lau：word2vec词之间的similarity我的理解是更像是关联度而不是相似度，也就是他们常常一起出现，所以引申出来他们有某种相似性

- [论文]《Random Walks on Context Spaces: Towards an Explanation of the Mysteries of Semantic Word Embeddings》(2015) S Arora, Y Li, Y Liang等 http://t.cn/RwHPhwf 用对数线性生成模型为word2vec等word embedding方法寻求合理解释。简化版解释一篇:《Why word2vec works》http://t.cn/Rw6jNo6

- [IPN]《Document classification by inversion of distributed language representations》http://t.cn/RwBr14V 基于Gensim(word2vec)的(评论)文档分类实例

- 中英文维基百科语料上的Word2Vec实验: 最近利用gensim word2vec模块在中英文维基百科语料上分别测试了一下word2vec，记录一下实验的过程，欢迎观摩 http://t.cn/Rwd87KO

## 稀疏表示

- 一篇对稀疏表示进行大量总结的综述性文章，A survey of sparse representation: Algorithms and applications, 发表不到一个月，下载量排名在所发表期刊的前十！ http://t.cn/RLUozO8

## 加强学习
- DeepMind在Nature上发表《Human-level control through deep reinforcement learning》http://t.cn/Rw0ZSW0 基于reinforcement learning让算法学习打游戏，源码:http://t.cn/Rw084MA ，http://t.cn/RwORUqX 对相关文章和工作进行了整理，DeepMind的专访http://t.cn/RwOR5EG 云:http://t.cn/RwORY9h

	论文PDF全文可从这里下：http://t.cn/RwOBdZ7

## 生物学
- Yoshua Bengio团队（Bengio一作）新作：Towards Biologically Plausible Deep Learning，尝试在Deep Learning与生物学观察（Spike-Timing- Dependent Plasticity）之间建立联系。寻找或探索机器学习的生物学解释，感觉很有意义。http://t.cn/RwCfROu

## 语音识别
- [文章]《Deep Speech: Accurate Speech Recognition with GPU-Accelerated Deep Learning》http://t.cn/RwHfNKN GPU加速的大规模深度学习，用于语音识别，来自百度硅谷AI实验室


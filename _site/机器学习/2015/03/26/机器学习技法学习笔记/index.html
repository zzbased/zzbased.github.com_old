
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="author" content="vincentyao" />
    <title>机器学习技法学习笔记</title>

    <link rel="stylesheet" href="/assets/themes/dinky/css/styles.css">
    <link rel="stylesheet" href="/assets/themes/dinky/css/pygment_trac.css">
    <script src="/assets/themes/dinky/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    



  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header"><a href="/">100的技术博客</a></h1>
        <p class="header">广告、算法与金融</p>
        <ul>
          
          
          


  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/categories.html">Categories</a></li>
      	
      
    
  
    
  
    
      
    
  
    
      
      	
      	<li><a href="/pages.html">Pages</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  



        </ul>
        
        
        <div class="misc vcard">
          <h4>about</h4>
          <ul>
            
            <li class="contact"><address><span class="author fn n">vincentyao</span> - <span class="fn email">zerobased@foxmail.com</span></address></li>
            
            
            <li class="github"><a href="http://github.com/zzbased/" rel="me">github.com/zzbased</a></li>
            
            
            <li class="twitter"><a href="http://weibo.com/zerobased/" rel="me">weibo.com/zerobased</a></li>
            
            
            <li class="twitter"><a href="http://twitter.com/callyling/" rel="me">twitter.com/callyling</a></li>
            
            
            
            
            
          </ul>
        </div><!-- misc -->
        
      </header>

      <section>
        
<section>
  <h1>机器学习技法学习笔记</h1>

  
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="section">机器学习技巧 学习笔记</h1>

<p>有用链接：</p>

<ul>
  <li><a href="https://www.coursera.org/course/ntumlone">机器学习基石</a></li>
  <li><a href="https://class.coursera.org/ntumltwo-001/lecture">机器学习技法</a></li>
  <li><a href="http://beader.me/mlnotebook/">beader.me笔记</a></li>
  <li><a href="http://www.douban.com/doulist/3440234/">听课笔记douban</a></li>
  <li><a href="http://mooc.guokr.com/course/610/機器學習基石--Machine-Learning-Foundations-/">mooc学院</a></li>
</ul>

<h2 id="linear-support-vector-machines">第1讲 Linear Support Vector Machines</h2>

<p>我们的目标是：最大间隔</p>

<p>求一个点x距离一个平面的距离：</p>

<table>
  <tbody>
    <tr>
      <td>点x到平面上的点x’的向量 x-x’，在平面的法向量上的投影：w*(x-x’)/</td>
      <td>w</td>
      <td>，即</td>
      <td>w^T*x+b</td>
      <td>/</td>
      <td>w</td>
      <td>。</td>
    </tr>
  </tbody>
</table>

<p>最大化这个距离，可以假设 min{y*(wx+b)}=1。那么目标变为：</p>

<table>
  <tbody>
    <tr>
      <td>max 1/</td>
      <td>w</td>
      <td>条件是： min{y*(wx+b)}=1</td>
    </tr>
  </tbody>
</table>

<p>进一步推导，得到最终优化的目标：</p>

<p>min 1/2 w*w^T  subject to y(wx+b)&gt;=1</p>

<p>这就是支持向量机的优化目标，它的损失函数，等同于： max{0, 1-ywx}</p>

<p>注意：函数间隔与几何间隔。</p>

<p>可以将这个优化目标转化到 <a href="http://cn.mathworks.com/discovery/quadratic-programming.html">二次规划 quadratic programming</a></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/quadratic_programming.png" alt="" /></p>

<p>large-margin algorithm的VC维分析。因为large margin的限制，相比于较于PLA，svm的dichotomies会更少。所以从VC维看，相比于PLA，其泛化能力更强。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/vc_dimension_of_large_margin_algorithm.png" alt="" /></p>

<p>large-margin hyperplanes：参数最少，所以boundary最简单。
一般的hyperplanes：参数适中，边界简单。
一般的hyperplanes+feature转换(非线性的)：参数较多，边界复杂。
large-margin hyperplanes+feature transform：则可以得到适中的参数个数，复杂的边界。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Benefits-of-Large-Margin-Hyperplanes.png" alt="" /></p>

<p><strong>扩展阅读</strong>
<a href="http://blog.csdn.net/v_july_v/article/details/7624837">支持向量机通俗导论（理解SVM的三层境界）</a></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter1_question1.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter1_question2.png" alt="" /></p>

<h2 id="dual-support-vector-machine">第2讲 Dual support vector machine</h2>

<p>讨论： Support Vector Classification，Logistic Regression，Support Vector Regression的区别：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-L1-and-L2-loss-Support-Vector-Classification.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-Logistic-Regression.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L1-regularized-L2-loss-Support-Vector-Classification.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L1-regularized-Logistic-Regression.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/L2-regularized-L1-and-L2-loss-Support-Vector-Regression.png" alt="" /></p>

<p>复习一下第1讲，直接求解SVM的original问题，利用QP方法，需要求解 d+1个变量(d指代feature转换后的维度)，N个约束条件。如果我们采用一个非线性变换，维度特别高，就不太可解了，所以我们想SVM without d。所以有 ‘Equivalent’ SVM: based on some dual problem of Original SVM。</p>

<p>这时就要用到lagrange multipliers。这里看下正则化，为什么正则化的表达式是这样的，这是通过lagrange multipliers。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Lagrange-Multipliers-regularization.png" alt="" /></p>

<p>下面是SVM的对偶问题推导过程：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Lagrange-Function1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/lagrange-dual-problem2.png" alt="" /></p>

<p>这里要提一下KKT条件：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/kkt_11.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/kkt_12.png" alt="" /></p>

<p>经过一通推导，我们得到了svm的对偶问题：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Dual-Formulation-of-svm.png" alt="" /></p>

<p>这个对偶问题，就可以用QP来求解了。</p>

<p>求得a后，primal问题的w和b，可以通过下面式子求得：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/w_b_optim.png" alt="" /></p>

<p>最后说一个解释：当a_n大于0时，此时该点正好处于边界上，这也就是所谓的支撑向量。</p>

<p>有趣之处在于，对于新点x的预测，只需要计算它与训练数据点的内积即可（表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。</p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question1.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question2.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question3.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter2_question4.png" alt="" /></p>

<h2 id="section-1">第3讲</h2>
<p>为什么要把SVM转换到对偶问题，原因有这样几个：1.对偶问题的变量为N个，有时候N远远小于d。2.解释了support vector。 3.比较直观的引入了核函数。</p>

<p>在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。</p>

<p>建立非线性学习器分为两步：
首先使用一个非线性映射将数据变换到一个特征空间F，
然后在特征空间使用线性学习器分类。</p>

<p>核函数的优势在于：
一个是映射到高维空间中，然后再根据内积的公式进行计算；
而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Kernel-SVM-with-QP.png" alt="" /></p>

<p>多项式核：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Poly-2-Kernel.png" alt="" /></p>

<p>SVM + Polynomial Kernel: Polynomial SVM</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Poly-Kernel.png" alt="" /></p>

<p>高斯核：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_kernel.png" alt="" /></p>

<p>看一下高斯核参数改变带来的变化：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_kernel2.png" alt="" /></p>

<p>下面对比一下常用的几种核函数：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_linear_kernel.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_poly_kernel.png" alt="" />
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_gaussian_kernel.png" alt="" /></p>

<p>当然，除了上面三种常用的核函数外，还可以自己构造一些核，只需要这些核满足mercer’s condition。不过需要说明的，很难。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/valid_kernel_2.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter3_question1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter3_question2.png" alt="" /></p>

<h2 id="section-2">第4讲</h2>

<p>使用松弛变量处理 outliers 方法，本讲的内容。</p>

<h2 id="blending-and-bagging">第6讲  Blending and Bagging</h2>

<p>Aggregation的方法包括：select, mix uniformly, mix non-uniformly, combine;
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/aggregation_1.png" alt="" /></p>

<p>为什么Aggregation方法是有效的？可以从两方面来看，其一通过Aggregation可以生成复杂的hypotheses，相当于做了feature transform；其二，生成的G(x)更加moderate，例如下图中PLA的uniform mix就是large-margin，相当于做了regularization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/aggregation_works.png" alt="" /></p>

<p><strong>uniform blending</strong></p>

<p>如果是classification，则有：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_for_classification.png" alt="" /></p>

<p>如果是regression，则有：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_for_regression.png" alt="" /></p>

<p>从上图还可以看出：任意g的Eout平均大于等于G的Eout。</p>

<p>从上图的公式还可以得出，expected performance of A = expected deviation to consensus +performance of consensus。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/uniform_blending_reduce_variance.png" alt="" /></p>

<p><strong>Linear Blending</strong></p>

<p>linear blending就像two-level learning。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear_blending.png" alt="" /></p>

<p>like selection, blending practically done with (Eval instead of Ein) + (gt− from minimum Etrain)</p>

<p>Any blending也叫Stacking。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/any_blending.png" alt="" /></p>

<p><strong>bagging</strong></p>

<p>aggregation里最重要的一个点就是：diversity。diversity的方法有很多种。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/diversity_important.png" alt="" /></p>

<p>下面介绍一种通过data randomness的方法，也叫bootstrapping，即bagging。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/bootstarpping1.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter7_question1.png" alt="" /></p>

<h2 id="adaptive-boosting">第8讲 Adaptive Boosting</h2>

<p>课程的最开始有一个分辨苹果的例子。以后AdaBoost的时候可以借鉴那个例子。其基本思路是：给予上次分错的样本更高的权重。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/re_weighting_bootstrapping.png" alt="" /></p>

<p>给每个example不同的weight，类似于给予不同的class的样本不同的weight。回忆一下，有时候我们false reject尽可能低，那对于这一类，我们在error measure给予更高的权重。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/re_weighting_bootstrapping2.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/false-accept-and-false-reject.png" alt="" /></p>

<p>具体怎么更新下一次训练的样本权重呢，参考下面的图：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/scaling_factor.png" alt="" /></p>

<p>有了样本权重更新公式后，则有一个Preliminary算法：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost_preliminary.png" alt="" /></p>

<p>得到这么多的g后，怎么得到G，也就是aggregation的方法，我们希望在计算g的时候把aggregation的权重也得到。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost1.png" alt="" /></p>

<p>那么完整算法为：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/adaboost2.png" alt="" /></p>

<p>下面是一些理论：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Theoretical-Guarantee-of-AdaBoost.png" alt="" /></p>

<p>Decision Stump</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Decision-Stump.png" alt="" /></p>

<p>AdaBoost与Decision Stump的结合 – &gt; AdaBoost-Stump:
efficient feature selection and aggregation</p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter8_question1.png" alt="" /></p>

<h2 id="decision-tree">第9讲 Decision Tree</h2>

<p>decision tree的位置，模仿人脑决策过程。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/decision-tree1.png" alt="" /></p>

<p>decision tree缺点：(1)启发式的规则(前人的巧思)，缺乏理论基础；(2)启发式规则很多，需要selection；(3)没有代表性的算法。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Disclaimers-about-Decision-Tree.png" alt="" /></p>

<p>一个基本的decision tree算法：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/basic-decision-tree.png" alt="" /></p>

<p>CART: classification and regression tree。
有两个简单的选择：binary tree；叶子节点是常数。</p>

<p>怎么选择branching，切完后两个子树的纯度最高。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Purifying-in-CART.png" alt="" /></p>

<p>怎么考量”不纯度”</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Impurity-Functions.png" alt="" /></p>

<p>最终CART算法如下：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/cart_algorithm.png" alt="" /></p>

<p>关于CART算法的演算过程，具体请参考 <a href="http://mydisk.com/yzlv/webpage/datamining/xiti.html">决策树算法的计算过程演示</a>，<a href="http://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a>，<a href="http://www.academia.edu/7032069/An_example_of_calculating_gini_gain_in_CART">An example of calculating gini gain in CART</a></p>

<p>几种决策树算法的区别：</p>

<p>C4.5算法是在ID3算法的基础上采用信息增益率的方法选择测试属性。 ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，又出现了根据GINI系数来选择测试属性的决策树算法CART。
CART算法采用一种二分递归分割的技术，与基于信息熵的算法不同，CART算法对每次样本集的划分计算GINI系数，GINI系数，GINI系数越小则划分越合理。CART算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。</p>

<p>Regularization</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Regularization-by-Pruning.png" alt="" /></p>

<p>当有categorical features时，CART也可以灵活处理。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Branching-on-Categorical-Features.png" alt="" /></p>

<p>如果有缺失特征的话，怎么办？可以利用surrogate feature。</p>

<p>看一个CART的例子：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/cart_example.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter9_question1.png" alt="" /></p>

<h2 id="random-forest">第10讲 random forest</h2>
<p>Bagging and Decision Tree，将这两者合在一起，就是Random forest。</p>

<p>random forest (RF) = bagging + fully-grown C&amp;RT decision tree</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Bagging-and-Decision-Tree.png" alt="" /></p>

<p>三个优点：</p>

<ul>
  <li>highly parallel/efficient to learn</li>
  <li>inherit pros of C&amp;RT</li>
  <li>eliminate cons of fully-grown tree</li>
</ul>

<p>因为是random forest，除了在bootstrapping时利用data randomness，还可以randomly sample d’ feature from x。即original RF re-sample new subspace for each b(x) in C&amp;RT。</p>

<p>那么更进一步了，RF = bagging + random-subspace C&amp;RT</p>

<p>random-combination的意思是：随机抽样一些features后，line combination，作为一个新的feature切分点。那么original RF consider d′ random low-dimensional projections for each b(x) in C&amp;RT。</p>

<p>所以，再进一步：RF = bagging + random-combination C&amp;RT</p>

<p>从上面可以看出，randomness是随处不在的。</p>

<p>回顾一下bagging的过程，每次随机抽样一些数据，这样下去，总会有一些样本是一直未被抽中的。未被抽中的概率计算为：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/numbers_of_oob.png" alt="" /></p>

<p>有了这些out-of-bag (OOB) examples后，可以将其作为validation set来使用。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/oob_vs_validation.png" alt="" /></p>

<p>那么，相比于原来的validation过程，RF可以做self-validation，也就是在训练的过程中，把model选择顺便也做了。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/model_selection_by_oob.png" alt="" /></p>

<p>接着看下Feature selection，decision tree正好是一个内建的feature selection过程。</p>

<p>先看下利用linear model做feature importance判别，训练完的模型，weight越大，表示feature越重要。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Feature_Selection_by_Importance.png" alt="" /></p>

<p>而RF可以采用permutation test来做特征选择。所谓permutation test，也就是对某一个特征，对所有样本上该维度的特征值做随机排列，然后在这个样本集上计算RF performance。用原来的performance减去这个新的performance后，就得到该特征的重要性。如下图所示：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/feature_selection_by_permutation_test.png" alt="" /></p>

<p>但是在RF上，因为OOB的存在，可以利用Eoob(G)-Eoob^p(G)。Eoob^p(G)是通过在OOB上permute某一维特征值。<strong>这里后续可以再深挖</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/feature_importance_by_rf.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter10_question1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter10_question2.png" alt="" /></p>

<h2 id="gradient-boosted-decision-treegbdt">第11讲 Gradient Boosted Decision Tree(GBDT)</h2>

<p>random forest用一句话来总结，则是：bagging of randomized C&amp;RT trees with automatic validation and feature selection。</p>

<p>比较一下Random forest和AdaBoost Tree。
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/compare_rd_adaboost-tree.png" alt="" /></p>

<p>但是要做AdaBoost Tree的话，首先需要weighted DTree。这个在LR,SVM等模型上容易做到，但是在DT上很难。所以我们换个思路，如果我们想给某个样本加个weight，可以在sample样本的时候，增大或者减小它的概率即可。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Randomized-Base-Algorithm.png" alt="" /></p>

<p>所以AdaBoost-DTree的组成由下图所示：
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/AdaBoost-DTree1.png" alt="" /></p>

<p>在adaboost算法中，如果一个g的错误率为0的话，那么这个g的权重将是无限大的。而在决策树的世界里，如果是full-grown的话，在训练数据上，错误率为0是很容易办到的。</p>

<p>那么为了避免这种过拟合的情况存在，我们需要对DT做剪枝。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/AdaBoost-DTree2.png" alt="" /></p>

<p>当我们extremely剪枝时，譬如限制树的高度小于等于1，那此时DT就变成了decision stump。所以有了adaboost-stump算法，它是AdaBoost-DTree的一种特例。</p>

<p>2，3，4节 未完待续。</p>

<p>更多具体的内容，请参考单独的文章： <a href="http://zzbased.github.io/2015/04/03/Aggregation模型.html">Aggregation模型</a></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter11_question1.png" alt="" /></p>

<h2 id="section-3">第12讲 神经网络</h2>

<p><strong>Motivation</strong></p>

<p>通过”Linear Aggregation of Perceptrons”，可以完成AND，OR，NOT等操作，可以完成 convex set等操作，但是不能完成XOR操作。怎么办？只能multi-layer perceptron。</p>

<p>XOR(g1, g2) = OR(AND(−g1, g2), AND(g1, −g2))</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/perceptron_powerful_limitation.png" alt="" /></p>

<p>perceptron (simple)
=⇒ aggregation of perceptrons (powerful)
=⇒ multi-layer perceptrons (more powerful)</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question1.png" alt="" /></p>

<p><strong>Neural Network Hypothesis</strong></p>

<p>output：any linear model can be used；
transformation function of score (signal) s：不用linear，因为多层线性=&gt;whole network linear。也不用阶梯函数(0-1)，因为它不可微。通常的选择有tanh(x)，sigmoid(s)。</p>

<p>tanh(x) = [exp(s)-exp(-s)] / [exp(s)+exp(-s)] = 2sigmoid(2x)-1</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question2.png" alt="" /></p>

<p><strong>Backpropagation</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Backpropagation_1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Backpropagation_2.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question3.png" alt="" /></p>

<p><strong>Optimization</strong></p>

<p>当multiple hidden layers，一般都是non-convex。对于最优化来说，不容易求得全局最优解。GD/SGD可能只能求出局部最优解。</p>

<p>对Wij做不同的初始化，可能有不同的局部最优解。所以对初始化值比较敏感。</p>

<p>有效的建议是：不要初始化太大的weights，因为large weight，加上tanh后，将saturate。如果做梯度下降的话，那段区域里有small gradient。所以建议要try some random&amp;small ones。</p>

<p>神经网络的dVC=O(VD)，V表示神经元的个数，D表示weight的个数，也就是edge的数目。</p>

<p>VC维太大，容易overfit。可以加一个L2 regularizer。但是加L2后，带来的只是shrink weights。我们希望可以得到sparse解，那么就可以用L1 regularizer，但L1不可微分。
所以另外一个选择是：weight-elimination（scaled L2），即large weight → median shrink; small weight → median shrink</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/weight-elimination-regularizer.png" alt="" /></p>

<p>Early Stopping，随着t 增长，VC维越大。所以合适的t 就够了。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/BP-Early-Stopping.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter12_question4.png" alt="" /></p>

<h2 id="deep-learning">第13讲 Deep Learning</h2>

<p>structural decisions: key issue for applying NNet。模型结构很关键。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Challenges-and-Key-Techniques-for-Deep-Learning.png" alt="" /></p>

<p>hinton 2006提出的：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/A-Two-Step-Deep-Learning-Framework.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Information-Preserving-Neural-Network.png" alt="" /></p>

<p>Auto-encoder的作用：监督学习的话，给予做特征；无监督学习的话，用来做密度预测，也可以用来做异常点检测。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Deep-Learning-with-Autoencoders.png" alt="" /></p>

<p>Regularization in Deep Learning的方法：</p>

<ul>
  <li>structural decisions/constraints，譬如卷积神经网络，循环神经网络</li>
  <li>weight decay or weight elimination regularizers</li>
  <li>Early stopping</li>
  <li>dropout，dropconnect等</li>
  <li>denosing</li>
</ul>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/denosing_auto-encoder.png" alt="" /></p>

<p>Linear Autoencoder Hypothesis
<img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear-autoencoder-hypothesis.png" alt="" /></p>

<p>简单点看就是：h(x) = WW^T x</p>

<p>复习一下特征值和特征向量。<a href="http://zh.wikipedia.org/wiki/特征向量">特征向量wiki</a></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/optimal_v_linear_autoencoder.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/pca-for-autoencoder.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter13_question1.png" alt="" /></p>

<h2 id="radial-basis-function-network">第14讲 Radial Basis Function Network</h2>

<p>以前在讲SVM时，有提到RBF kernel(gaussian kernel)，这里回顾一下。高斯核是将x空间变换到z空间的无限维。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_svm.png" alt="" /></p>

<p>基于高斯核的SVM如下所示，相当于是support vector上的radial hypotheses的线性组合。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/gaussian_svm2.png" alt="" />
So，Radial Basis Function (RBF) Network: linear aggregation of radial hypotheses。</p>

<p>将RBF network类比于neural network，output layer是一样的，都是线性组合，不一样是隐藏层(在RBF network里，是distance + gaussian)。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/rbf_network.png" alt="" /></p>

<p>基于RBF network来解释SVM：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/RBF-Network-Hypothesis.png" alt="" /></p>

<p>此时，需要学习的参数是 u_m(是centers)，b_m(是不同rbf线性组合的系数)。</p>

<p>kernel是Z空间的相关性度量，而RBF是X空间的相关性度量。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/rbf_vs_kernel.png" alt="" /></p>

<p>所以RBF network： distance similairty-to-centers as feature transform。</p>

<p>Full RBF Network：是说将所有样本点都参与到运算里(M=N)。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/full_rbf_network.png" alt="" /></p>

<p>full rbf network是一个lazy way to decide u_m。</p>

<p>Nearest-Neighbor：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Nearest-Neighbor.png" alt="" /></p>

<p>如果是利用RBF network做regression呢？如下所示。但是这样做了后，Ein(g)=0，这样势必会overfit。
所以需要做正则化。正则化的思路有：(1)类似于kernel ridge regression，加正则项。(2)fewer centers，譬如support vector。constraining number of centers and voting weights。</p>

<p>那怎样才能做到fewer centers呢？通常的方法就是：寻找prototypes。那how to extract prototypes?</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Regularized-Full-RBF-Network.png" alt="" /></p>

<p>这里提到一种算法：k-means cluster。k-means的优化思路为：alternating minimization。说到这，EM也属于alternating minimization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/k-Means-Algorithm.png" alt="" /></p>

<p>OK，现在prototypes提取到了，接下来把基于k-means的rbf network写出来。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/RBF-Network-Using-k-Means.png" alt="" /></p>

<p>下面是实战，先看一个k-means的例子：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/k-means-examples.png" alt="" /></p>

<p>可以看到，k和初始化，在k-means算法里非常关键。</p>

<p>通常随机地从样本中挑k个出来作为k个初始的聚类中心。但这不是个明智的选择。它有可能会导致图像趋于稠密聚集某些区域，因为如果训练样本本身就在某个区域分布非常密，那么我们随机去选择聚类中心的时候，就会出现就在这个数据分布密集的地方被选出了很多的聚类中心。</p>

<p>那么怎么做k-means的初始化呢？</p>

<ul>
  <li>多次运行数据集合，选择最小的SSE的分簇结果作为最终结果。该方法依赖于数据集合和簇数量，对分簇结果有比较大影响，所以在某些场景下效果也不是很好。</li>
  <li>抽取数据集合样本，对样本进行Hierarchical Clustering技术，从中抽取K个Clustering作为初始中心点。该方法工作良好，知识有两点限制条件：抽样数据不能太大，因为Hierarchical Clustering比较耗时间；K值相对于抽样数据比较小才行。</li>
  <li>kmeans++算法。<a href="http://en.wikipedia.org/wiki/K-means%2B%2B">kmeans++ wiki</a>，<a href="http://www.cnblogs.com/shelocks/archive/2012/12/20/2826787.html">kmenas++中文</a>。</li>
</ul>

<p>K-means++的步骤为：</p>

<ol>
  <li>从输入的数据点集合中随机选择一个点作为第一个聚类中心</li>
  <li>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</li>
  <li>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</li>
  <li>重复2和3直到k个聚类中心被选出来</li>
  <li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ol>

<p>更多关于k-means初始化的方法，请参考 <a href="http://www.mecs-press.org/ijisa/ijisa-v4-n1/IJISA-V4-N1-3.pdf">Efficient and Fast Initialization Algorithm for K- means Clustering</a></p>

<p><strong>扩展阅读</strong></p>

<ul>
  <li>
    <p>发表在Science的论文：基于密度的快速无监督聚类方法 <a href="http://t.cn/RAASZ4q">Clustering by fast search and find of density peaks. A Rodriguez, A Laio (2014) </a> 很棒，推荐给没看过的朋友，另有相关中文两篇：http://t.cn/RPoKmOi http://t.cn/RPOs6uK 供参考理解 云:http://t.cn/RAACowz</p>

    <p>对所有坐标点，基于相互距离，提出了两个新的属性，一是局部密度rho，即与该点距离在一定范围内的点的总数，二是到更高密度点的最短距离delta。作者提出，类簇的中心是这样的一类点：它们被很多点围绕（导致局部密度大），且与局部密度比自己大的点之间的距离也很远。</p>
  </li>
  <li>
    <p>Canopy 聚类算法的基本原则是：首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个Canopy。Canopy 之间可以有重叠的部分。然后采用严格的距离计算方式准确的计算在同一 Canopy 中的点，将他们分配与最合适的簇。Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。<a href="http://blog.pureisle.net/archives/2045.html">Clustering Algorithm/聚类算法</a>，<a href="http://en.wikipedia.org/wiki/Canopy_clustering_algorithm">Canopy clustering algorithm</a>。</p>
  </li>
  <li>
    <p><a href="http://t.cn/RAwxOJx">文章 K-means Clustering with scikit-learn</a> PyData SV 2014上Sarah Guido的报告，Python下用Scikit-Learn做K-means聚类分析的深入介绍，涉及k值选取、参数调优等问题，很实用 GitHub:http://t.cn/RAwxsFS 云(视频+讲义):http://t.cn/RAwJJkG</p>
  </li>
  <li>
    <p><a href="http://t.cn/RwlDlgq">文章 Divining the ‘K’ in K-means Clustering</a> 用G-means算法确定K-means聚类最佳K值，G-means能很好地处理stretched out clusters(非球面伸展型类簇)</p>
  </li>
  <li>
    <p><a href="http://www.cc.gatech.edu/~isbell/tutorials/rbf-intro.pdf">RBF的核心论文 Introduction to Radial Basis Function Networks</a></p>
  </li>
</ul>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2_chapter14_question1.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2_chapter14_question2.png" alt="" /></p>

<h2 id="matrix-factorization">第15讲 Matrix Factorization</h2>

<p>从”Linear Network” Hypothesis说起，用来做推荐，也就是根据feature x，预测得分y。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Linear-Network-Hypothesis.png" alt="" /></p>

<p>求解上面的linear network，采用squared error。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/linear_model_for_recommendation.png" alt="" /></p>

<p>从上面的求解过程，得到Matrix factorization：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Matrix-Factorization1.png" alt="" /></p>

<p>具体下来，应该怎么求解呢？考虑到这里面有两个变量W和V，这时可以采用alternating minimization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Matrix-Factorization-Learning.png" alt="" /></p>

<p>所以，得到Alternating Least Squares方法。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Alternating-Least-Squares.png" alt="" /></p>

<p>比较一下 linear autoencoder 和 matrix factorization。linear autoencoder
≡ special matrix factorization of complete X</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Linear-Autoencoder-versus-Matrix-Factorization.png" alt="" /></p>

<p>上面讲述了 alternating解法，matrix factorization还可以利用Stochastic gradient descent求解。SGD：most popular large-scale matrix factorization algorithm，比alternating速度更快。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/SGD-for-Matrix-Factorization.png" alt="" /></p>

<p>举一个SGD的例子。在KDDCup 2011 Track1中，因为该推荐任务与时间系列有关，所以在优化时，没有用stochastic GD算法，而是采用了time-deterministic GD算法，也就是最近的样本最后参与计算，这样可以保证最近的样本拟合得更好。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/KDDCup-2011-Track1.png" alt="" /></p>

<h2 id="extraction-models">Extraction Models总结</h2>

<p>将特征转换纳入到我们的学习过程。</p>

<p>Extraction Models： neural network，RBF network，Matrix Factorization。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Map-of-Extraction-Models.png" alt="" /></p>

<p>Extraction Techniques：function gradient descnet，SGD。</p>

<p>无监督学习用于预训练，例如autoencoder，k-means clustering。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Map-of-Extraction-Techniques.png" alt="" /></p>

<p>regularization：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Pros-and-Cons-of-Extraction-Models.png" alt="" /></p>

<h2 id="finale-">第16讲 Finale 大总结</h2>

<p>Exploiting Numerous Features via Kernel：Polynomial Kernel，Gaussian Kernel等。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Numerous-Features-via-Kernel.png" alt="" /></p>

<p>Exploiting Predictive Features via Aggregation：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Predictive-Features-via-Aggregation.png" alt="" /></p>

<p>Exploiting Hidden Features via Extraction：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Hidden-Features-via-Extraction.png" alt="" /></p>

<p>Exploiting Low-Dim. Features via Compression：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/Exploiting-Low-Dim.Features-via-Compression.png" alt="" /></p>

<p><strong>习题</strong></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/mlfoundation_learn/2chapter16_question1.png" alt="" /></p>


  
</section>


      </section>

      <footer>
        <p><small>Hosted on <a href="http://pages.github.com/">GitHub Pages</a> using the <a href="https://github.com/sodabrew/theme-dinky">Dinky theme</a> for <a href="http://jekyllbootstrap.com/">Jekyll Bootstrap</a></small></p>
      </footer>

    </div>
    <!--[if !IE]><script>fixScale(document);</script><!--<![endif]-->
  </body>
</html>

